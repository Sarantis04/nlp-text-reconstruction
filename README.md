# Natural Language Processing : Text Reconstruction
## Description
A complete NLP project for text reconstruction and evaluation using rule-based rewriting, pre-trained language model pipelines, semantic similarity analysis, and automatic metrics like cosine similarity, PCA/t-SNE visualization, and BERTScore.

## ðŸ“Œ Features
- âœ… Rule-based text rewriting  
- ðŸ¤– Grammar correction and summarization with pre-trained transformers  
- ðŸ“Š Semantic similarity analysis (cosine similarity, embeddings)  
- ðŸ§ª Visualization with PCA and t-SNE  
- ðŸ“ˆ BERTScore evaluation vs. gold standards

## ðŸ¤– Models & Tools Used
The project makes use of several pre-trained NLP models and libraries:
- Rule-based rewriting with Python (re, nltk) for controlled sentence corrections.
- vennify/t5-base-grammar-correction â€“ Transformer model for grammar correction.
- prithivida/grammar_error_correcter_v1 â€“ Grammar error correction model.
- sshleifer/distilbart-cnn-12-6 â€“ Summarization model for generating concise versions of texts.
- Sentence-Transformers (all-MiniLM-L6-v2) â€“ For semantic embeddings and cosine similarity analysis.
- BERTScore â€“ For evaluation against gold standard texts.
- scikit-learn (PCA, t-SNE) â€“ For dimensionality reduction and visualization of embeddings.

## ðŸš€ Usage
Run each notebook step by step to reproduce results.

## ðŸ“Š Results
- Grammar-corrected text shows highest semantic similarity to gold standard
- Summarization yields concise but less faithful outputs
- Rule-based rewriting is useful for predictable error patterns and smaller texts
